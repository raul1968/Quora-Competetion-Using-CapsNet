{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "740fcaa1d69d8a2dc231f9a5df7c45af1f66d19d"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom tensorflow.python.keras.models import Sequential\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, SpatialDropout1D\nfrom keras.layers import *\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom tensorflow.python.keras.optimizers import Adam\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import *\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import *\nfrom keras.models import *\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.initializers import *\nfrom keras.optimizers import *\nimport keras.backend as K\nfrom keras.callbacks import *\nimport tensorflow as tf\nimport os\nimport time\nimport gc\nimport re",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4614edd904d61aefb8ef0cfc0ba825aa9900e76f",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "df_train = pd.read_csv(\"../input/train.csv\")\ntext_data = df_train[\"question_text\"].values\ntarget = df_train[\"target\"].values\ndf_test = pd.read_csv(\"../input/test.csv\")\ntest_data = df_test[\"question_text\"].values\nX_train, X_test, y_train, y_test = train_test_split(text_data,target, random_state = 23, test_size=0.2)\nnumWords = 20000\ntokenizer = Tokenizer(num_words = numWords)\ntokenizer.fit_on_texts(text_data)\nx_train_tokens = tokenizer.texts_to_sequences(X_train)\nx_test_tokens = tokenizer.texts_to_sequences(X_test)\n# len_features = [len(x) for x in x_train_tokens]\n# sum(x>50 for x in len_features)\npad = 'pre'\nmaxTokens = 50\nx_train_pad = pad_sequences(x_train_tokens, maxlen=maxTokens, padding=pad,\n                           truncating=pad)\nx_test_pad = pad_sequences(x_test_tokens, maxlen=maxTokens, padding=pad,\n                          truncating = pad)\ny_pred_lst = []",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "a78e5de1dad8538631beb570e13a122b708edcbb"
      },
      "cell_type": "code",
      "source": "# from tqdm import tqdm\n# inverse_transform = dict(zip(tokenizer.word_index.values(),tokenizer.word_index.keys()))\n# def convertTokensToString(tokens):\n#     words = [inverse_transform[token] for token in tokens if token!=0]\n#     return words\n# x_train_tokens_words = [convertTokensToString(x) for x in tqdm(x_train_tokens)]\n# x_test_tokens_words = [convertTokensToString(x) for x in tqdm(x_test_tokens)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ef73cddddbdcbe0bd96758fee15f22e420cef337"
      },
      "cell_type": "code",
      "source": "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\ndef generate_embeddings(EMBEDDING_FILE):\n    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n    all_embs = np.stack(embeddings_index.values())\n    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n    embed_size = all_embs.shape[1]\n    word_index = tokenizer.word_index\n    nb_words = min(numWords,len(word_index))\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n    for word, i in word_index.items():\n        if i >= numWords: continue\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n    return embed_size, embedding_matrix\n",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9b49256087d344a3507a12bbe7247497dc9fbfeb"
      },
      "cell_type": "code",
      "source": "#This cell is taken from a different kernel on kaggle\ndef squash(x, axis=-1):\n    # s_squared_norm is really small\n    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n    # return scale * x\n    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n    scale = K.sqrt(s_squared_norm + K.epsilon())\n    return x / scale\n\n# A Capsule Implement with Pure Keras\nclass Capsule(Layer):\n    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n                 activation='default', **kwargs):\n        super(Capsule, self).__init__(**kwargs)\n        self.num_capsule = num_capsule\n        self.dim_capsule = dim_capsule\n        self.routings = routings\n        self.kernel_size = kernel_size\n        self.share_weights = share_weights\n        if activation == 'default':\n            self.activation = squash\n        else:\n            self.activation = Activation(activation)\n\n    def build(self, input_shape):\n        super(Capsule, self).build(input_shape)\n        input_dim_capsule = input_shape[-1]\n        if self.share_weights:\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(1, input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     # shape=self.kernel_size,\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n        else:\n            input_num_capsule = input_shape[-2]\n            self.W = self.add_weight(name='capsule_kernel',\n                                     shape=(input_num_capsule,\n                                            input_dim_capsule,\n                                            self.num_capsule * self.dim_capsule),\n                                     initializer='glorot_uniform',\n                                     trainable=True)\n    def call(self, u_vecs):\n        if self.share_weights:\n            u_hat_vecs = K.conv1d(u_vecs, self.W)\n        else:\n            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n\n        batch_size = K.shape(u_vecs)[0]\n        input_num_capsule = K.shape(u_vecs)[1]\n        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n                                            self.num_capsule, self.dim_capsule))\n        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n\n        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n        for i in range(self.routings):\n            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n            c = K.softmax(b)\n            c = K.permute_dimensions(c, (0, 2, 1))\n            b = K.permute_dimensions(b, (0, 2, 1))\n            outputs = self.activation(tf.keras.backend.batch_dot(c, u_hat_vecs, [2, 2]))\n            if i < self.routings - 1:\n                b = tf.keras.backend.batch_dot(outputs, u_hat_vecs, [2, 3])\n\n        return outputs\n\n    def compute_output_shape(self, input_shape):\n        return (None, self.num_capsule, self.dim_capsule)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8171747f989f8e951ab0b9568c84194eb06598fd"
      },
      "cell_type": "code",
      "source": "# from collections import Counter\n# from imblearn.under_sampling import RandomUnderSampler\n# rus = RandomUnderSampler({0: 64625*3, 1: 64625},random_state=42)\n# X_res, y_res = rus.fit_resample(x_train_pad, y_train)\n# print('Resampled dataset shape %s' % Counter(y_res))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "942ee64229fcf4f4cf64fb7ec90bca2af6d8eb5b",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "def build_model():\n    max_features = numWords\n    maxlen=50\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n    x = SpatialDropout1D(rate=0.2)(x)\n    x = Bidirectional(CuDNNGRU(128, return_sequences=True,\n                               recurrent_initializer = orthogonal(gain=1.0,seed = 10000)))(x)\n    x = CuDNNGRU(26, return_sequences=True)(x)\n    x = Capsule(num_capsule = 10, dim_capsule = 10,\n            routings=4,\n            share_weights=True)(x)\n    x = Flatten()(x)\n    x = Dense(70, activation=\"relu\")(x)\n    x = Dropout(0.12)(x)\n    x = BatchNormalization()(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    print(model.summary())\n    model.fit(x_train_pad,y_train,validation_split=0.05,epochs=2,batch_size=512)\n    y_pred = model.predict(x_test_pad, verbose=1)\n    #y_pred_training = model.predict(x_train_pad, verbose=1)\n    return y_pred",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7b46a1e0e2ff6b1307ecc79abbe50cbf1628362c"
      },
      "cell_type": "code",
      "source": "import gc\nEMBEDDING_FILES = ['../input/embeddings/glove.840B.300d/glove.840B.300d.txt','../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec',\n                   '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt']\ny_pred_lst=[]\ny_pred_lst_training=[]\nfor EMBEDDING_FILE in EMBEDDING_FILES:\n    embed_size, embedding_matrix = generate_embeddings(EMBEDDING_FILE)\n    #y_pred, y_pred_training = build_model()\n    y_pred = build_model()\n    y_pred_lst.append(y_pred)\n    #y_pred_lst_training.append(y_pred_training)\n    del embedding_matrix\n    gc.collect()",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  after removing the cwd from sys.path.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 50)                0         \n_________________________________________________________________\nembedding_1 (Embedding)      (None, 50, 300)           6000000   \n_________________________________________________________________\nspatial_dropout1d_1 (Spatial (None, 50, 300)           0         \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 50, 256)           330240    \n_________________________________________________________________\ncu_dnngru_2 (CuDNNGRU)       (None, 50, 26)            22152     \n_________________________________________________________________\ncapsule_1 (Capsule)          (None, 10, 10)            2600      \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 100)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 70)                7070      \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 70)                0         \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 70)                280       \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 71        \n=================================================================\nTotal params: 6,362,413\nTrainable params: 6,362,273\nNon-trainable params: 140\n_________________________________________________________________\nNone\nTrain on 992652 samples, validate on 52245 samples\nEpoch 1/2\n992652/992652 [==============================] - 205s 207us/step - loss: 0.1425 - acc: 0.9460 - val_loss: 0.1052 - val_acc: 0.9583\nEpoch 2/2\n992652/992652 [==============================] - 202s 204us/step - loss: 0.1036 - acc: 0.9585 - val_loss: 0.1009 - val_acc: 0.9605\n261225/261225 [==============================] - 91s 349us/step\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  after removing the cwd from sys.path.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         (None, 50)                0         \n_________________________________________________________________\nembedding_2 (Embedding)      (None, 50, 300)           6000000   \n_________________________________________________________________\nspatial_dropout1d_2 (Spatial (None, 50, 300)           0         \n_________________________________________________________________\nbidirectional_2 (Bidirection (None, 50, 256)           330240    \n_________________________________________________________________\ncu_dnngru_4 (CuDNNGRU)       (None, 50, 26)            22152     \n_________________________________________________________________\ncapsule_2 (Capsule)          (None, 10, 10)            2600      \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 100)               0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 70)                7070      \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 70)                0         \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 70)                280       \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 71        \n=================================================================\nTotal params: 6,362,413\nTrainable params: 6,362,273\nNon-trainable params: 140\n_________________________________________________________________\nNone\nTrain on 992652 samples, validate on 52245 samples\nEpoch 1/2\n992652/992652 [==============================] - 203s 205us/step - loss: 0.1407 - acc: 0.9464 - val_loss: 0.1080 - val_acc: 0.9577\nEpoch 2/2\n992652/992652 [==============================] - 202s 203us/step - loss: 0.1032 - acc: 0.9588 - val_loss: 0.1041 - val_acc: 0.9588\n261225/261225 [==============================] - 96s 366us/step\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         (None, 50)                0         \n_________________________________________________________________\nembedding_3 (Embedding)      (None, 50, 300)           6000000   \n_________________________________________________________________\nspatial_dropout1d_3 (Spatial (None, 50, 300)           0         \n_________________________________________________________________\nbidirectional_3 (Bidirection (None, 50, 256)           330240    \n_________________________________________________________________\ncu_dnngru_6 (CuDNNGRU)       (None, 50, 26)            22152     \n_________________________________________________________________\ncapsule_3 (Capsule)          (None, 10, 10)            2600      \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 100)               0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 70)                7070      \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 70)                0         \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 70)                280       \n_________________________________________________________________\ndense_6 (Dense)              (None, 1)                 71        \n=================================================================\nTotal params: 6,362,413\nTrainable params: 6,362,273\nNon-trainable params: 140\n_________________________________________________________________\nNone\nTrain on 992652 samples, validate on 52245 samples\nEpoch 1/2\n992652/992652 [==============================] - 206s 207us/step - loss: 0.1431 - acc: 0.9460 - val_loss: 0.1058 - val_acc: 0.9572\nEpoch 2/2\n992652/992652 [==============================] - 203s 205us/step - loss: 0.1045 - acc: 0.9585 - val_loss: 0.1019 - val_acc: 0.9593\n261225/261225 [==============================] - 98s 373us/step\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e0b68623a36d38bd8b8e6ebcf686e64c4207521d"
      },
      "cell_type": "code",
      "source": "y_pred = 0.33*y_pred_lst[0] + 0.33*y_pred_lst[1] + 0.33*y_pred_lst[2]\nfrom sklearn.metrics import accuracy_score, f1_score\ndef get_thresh_f1(y_pred):\n    f1score_max = 0\n    thresh_max = 0\n    for thresh in np.arange(0.1, 0.501, 0.01):\n        thresh = np.round(thresh, 2)\n        if(f1score_max<f1_score(y_test, (y_pred>thresh).astype(int))):\n            f1score_max = f1_score(y_test, (y_pred>thresh).astype(int))\n            thresh_max = thresh\n    return f1score_max, thresh_max\nget_thresh_f1(y_pred)",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "(0.6678566203365034, 0.32)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb003f18289cc172cc1ddf3742460707cf432321"
      },
      "cell_type": "code",
      "source": "# df_submission = pd.DataFrame({\"qid\":df_test[\"qid\"].values, \"prediction\":y_pred_final})\n# df_submission = df_submission[[\"qid\",\"prediction\"]]\n# df_submission.to_csv(\"submission.csv\",index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a746b1de126a5ded283d05c4334052045adadabc"
      },
      "cell_type": "code",
      "source": "# y_pred = 0.33*y_pred_gnews + 0.33*y_pred_wiki + 0.33*y_pred_paragram",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}